
## Why XGBoost
- https://github.com/dmlc/xgboost
- http://datascience.la/xgboost-workshop-and-meetup-talk-with-tianqi-chen/

![image](https://user-images.githubusercontent.com/16873751/85156321-c1dd1800-b20e-11ea-8d2a-a4b1cb908f3b.png)

decision tree, weak learner(individually they are quite inaccurate, but slightly better when work together)

![image](https://user-images.githubusercontent.com/16873751/85156771-63646980-b20f-11ea-83e9-dcb2341a41cc.png)

second tree must provide positive effort when combine with first tree


## PCA

- https://setosa.io/ev/principal-component-analysis/

![image](https://user-images.githubusercontent.com/16873751/85156926-9c9cd980-b20f-11ea-859a-5da8e06157a5.png)


- Dimension for original data is reasonable for human beings, but might not friendly for decision tree

- Try to represent data with different `coordinate system`

- Make the dimension the principal component with most variation - (Minimize difference between min value and max value)

## Why Parquet



